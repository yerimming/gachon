{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yerimming/gachon/blob/main/CV_final_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   AdamP 사용\n",
        "*   lr = 0.001\n",
        "*   batch = 32\n",
        "*   CosineAnnealingLR 사용"
      ],
      "metadata": {
        "id": "sKSgDa9C1huU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iiYHcVfaZBfv",
        "outputId": "8165dc6e-41de-427a-fcb7-c93e09b27555"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install adamp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMZDR_TbZP8F",
        "outputId": "288d83fe-e736-487a-88f3-2dcad896bd63"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: adamp in /usr/local/lib/python3.10/dist-packages (0.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms import ToTensor,Normalize, RandomHorizontalFlip, Resize\n",
        "from torchvision.transforms import RandAugment\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from PIL import Image,ImageFilter,ImageEnhance\n",
        "\n",
        "from torch.utils.data import ConcatDataset\n",
        "from collections import Counter\n",
        "\n",
        "from adamp import AdamP\n",
        "\n",
        "# 스케줄러 사용\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "\n",
        "import re\n",
        "\n",
        "# 랜덤 시드 설정 공통\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "np.random.seed(seed)\n",
        "\n",
        "### GPU Setting ###\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
        "print(DEVICE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKpdYNIyZRZs",
        "outputId": "2f9607df-8e6f-4ccd-ae82-6c30da7bb971"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Custom Dataset ###\n",
        "class CUB2011(Dataset):\n",
        "  def __init__(self, transform, mode='train'):\n",
        "    self.transform = transform\n",
        "    self.mode = mode\n",
        "\n",
        "    if self.mode == 'train':\n",
        "      self.image_folder = os.listdir('/content/gdrive/MyDrive/CUB_200_2011_repackage_class50/datasets/train')\n",
        "    elif self.mode == 'valid':\n",
        "      self.image_folder = os.listdir('/content/gdrive/MyDrive/CUB_200_2011_repackage_class50/datasets/valid')\n",
        "    elif self.mode == 'test':\n",
        "      self.image_folder = os.listdir('/content/gdrive/MyDrive/CUB_200_2011_repackage_class50/datasets/test')\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.image_folder)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    img_path = self.image_folder[idx]\n",
        "    img = Image.open(os.path.join('/content/gdrive/MyDrive/CUB_200_2011_repackage_class50/datasets', self.mode, img_path)).convert('RGB')\n",
        "    img = self.transform(img)\n",
        "    label = img_path.split('_')[-1].split('.')[0]\n",
        "    label = re.sub(r'\\([^)]*\\)', '', label)\n",
        "    label = int(label)\n",
        "    return (img, label)"
      ],
      "metadata": {
        "id": "k9qK-HhcZe4v"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Geomentric transform + Visual corruptions\n",
        "class AddGaussianNoise(object):\n",
        "    def __init__(self, mean=0, std=1, p=0.5):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "        self.p = p\n",
        "\n",
        "    def __call__(self, img):\n",
        "        if np.random.rand() < self.p:\n",
        "            img_array = np.array(img)\n",
        "            noise = np.random.normal(self.mean, self.std, img_array.shape)\n",
        "            noisy_image = np.clip(img_array + noise, 0, 255)  # Clip values to the range [0, 255]\n",
        "            return Image.fromarray(noisy_image.astype(np.uint8))\n",
        "        else:\n",
        "            return img\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + f'(mean={self.mean}, std={self.std}, p={self.p})'\n",
        "\n",
        "\n",
        "class AdjustContrast(object):\n",
        "    def __init__(self, factor=1.0):\n",
        "        self.factor = factor\n",
        "\n",
        "    def __call__(self, img):\n",
        "        enhancer = ImageEnhance.Contrast(img)\n",
        "        img = enhancer.enhance(self.factor)\n",
        "        return img\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + f'(factor={self.factor})'\n",
        "\n",
        "class AdjustBrightness(object):\n",
        "    def __init__(self, factor=1.0):\n",
        "        self.factor = factor\n",
        "\n",
        "    def __call__(self, img):\n",
        "        enhancer = ImageEnhance.Brightness(img)\n",
        "        img = enhancer.enhance(self.factor)\n",
        "        return img\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + f'(factor={self.factor})'\n",
        "\n",
        "\n",
        "# Mix up\n",
        "def mixup_data(x, y, alpha=1.0):\n",
        "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
        "    if alpha > 0:\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "    else:\n",
        "        lam = 1\n",
        "\n",
        "    batch_size = x.size()[0]\n",
        "    index = torch.randperm(batch_size, device=x.device, dtype=torch.long)  # GPU에 있는 x.device를 사용하여 인덱스를 GPU로 전송\n",
        "\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
        "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)"
      ],
      "metadata": {
        "id": "WJO-iJmiZhra"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Data Preprocessing ###\n",
        "transforms_train_origin = transforms.Compose([transforms.Resize((448,448), Image.BICUBIC),\n",
        "                                       transforms.ToTensor(),\n",
        "                                       transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "                                       ])\n",
        "transforms_test = transforms.Compose([transforms.Resize((448,448), Image.BICUBIC),\n",
        "                                       transforms.ToTensor(),\n",
        "                                      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "                                       ])\n",
        "\n",
        "# Apply RandAugment\n",
        "transforms_train_rand = transforms.Compose([\n",
        "    transforms.Resize((448, 448), Image.BICUBIC),\n",
        "    RandAugment(5,3),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Geomentric transform + Visual corruptions\n",
        "transforms_train_g_v = transforms.Compose([\n",
        "    transforms.Resize((448, 448), Image.BICUBIC),\n",
        "    transforms.RandomResizedCrop(448),\n",
        "    transforms.RandomHorizontalFlip(0.5),\n",
        "    transforms.RandomVerticalFlip(0.5),\n",
        "    transforms.RandomRotation(30),\n",
        "    AddGaussianNoise(mean=0, std=25, p=0.5),  # 가우시안 노이즈를 추가합니다.\n",
        "    AdjustContrast(factor=2.0),  # 대비를 조절합니다.\n",
        "    AdjustBrightness(factor=1.5),  # 밝기를 조절합니다.\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_set_origin = CUB2011(mode='train',\n",
        "                    transform=transforms_train_origin)\n",
        "val_set = CUB2011(mode='valid',\n",
        "                  transform=transforms_test)\n",
        "test_set = CUB2011(mode='test',\n",
        "                  transform=transforms_test)\n",
        "\n",
        "# 데이터 증강을 위해 원래 데이터를 복사하고 추가\n",
        "train_set_augmented = CUB2011(mode='train', transform=transforms_train_rand)\n",
        "train_set_augmented2 = CUB2011(mode='train',transform=transforms_train_g_v)\n",
        "\n",
        "train_loader = DataLoader(train_set_origin,batch_size=BATCH_SIZE,shuffle=True)\n",
        "\n",
        "# Mixup된 이미지를 train_loader에 추가\n",
        "train_set_mixup = []\n",
        "for input, target in train_loader:\n",
        "    mixed_input, target_a, target_b, lam = mixup_data(input, target, alpha=1.0)\n",
        "    train_set_mixup.append((mixed_input, target_a, target_b, lam))\n",
        "train_set_mixup = transforms.Compose([transforms.Resize((448,448), Image.BICUBIC),\n",
        "                                       transforms.ToTensor(),\n",
        "                                      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "                                       ])\n",
        "train_set_mixup = CUB2011(mode='train', transform=train_set_mixup)\n",
        "\n",
        "\n",
        "# 각 클래스의 샘플 수를 계산\n",
        "class_counts = Counter([label for _, label in train_set_origin])\n",
        "\n",
        "# 가장 작은 클래스의 샘플 수를 찾아 minority_class_label로 지정\n",
        "minority_class_label = min(class_counts, key=class_counts.get)\n",
        "\n",
        "# Create a new dataset from replicated_data\n",
        "replicated_data = CUB2011(mode='train', transform=transforms_train_rand)\n",
        "replicated_data.image_folder = [data for data in train_set_origin.image_folder if data[1] == minority_class_label]\n",
        "\n",
        "\n",
        "\n",
        "# 두 데이터셋을 연결하여 새로운 훈련 데이터셋 생성\n",
        "train_set_combined = ConcatDataset([train_set_origin, replicated_data,\n",
        "                                    train_set_augmented,train_set_augmented2,train_set_mixup])\n",
        "\n",
        "print('Num of each dataset: ',len(train_set_combined),len(val_set),len(test_set))\n",
        "\n",
        "\n",
        "#-----------------------------------------------------------------------\n",
        "# Dataloader class는 bath기반의 딥러닝모델 학습을 위해서 mini batch를 만들어주는 역할을 한다\n",
        "# dataloader를 통해 dataset의 전체 데이터가 batch size로 나뉘게 된다\n",
        "train_loader = DataLoader(train_set_combined, batch_size=BATCH_SIZE, shuffle=True)\n",
        "# train_loader = DataLoader(train_set_combined, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_set,batch_size=BATCH_SIZE,shuffle=False)\n",
        "test_loader = DataLoader(test_set,batch_size=BATCH_SIZE,shuffle=False)\n",
        "\n",
        "print(\"Loaded dataloader\")\n"
      ],
      "metadata": {
        "id": "VxQAydvranCJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "686fc692-46cb-44d7-aa52-b5f6c4a55788"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num of each dataset:  9448 296 298\n",
            "Loaded dataloader\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Model / Optimizer ###\n",
        "EPOCH = 15\n",
        "lr = 0.001\n",
        "\n",
        "model = models.resnet18(pretrained=True)\n",
        "\n",
        "### Tranfer Learning ###\n",
        "num_features = model.fc.in_features\n",
        "model.fc = nn.Linear(num_features,50)\n",
        "model.to(DEVICE)\n",
        "\n",
        "optimizer = AdamP(model.parameters(),lr=lr,betas=(0.9, 0.999))\n",
        "\n",
        "# 스케줄러 초기화 (주기와 최소/최대 학습률 설정)\n",
        "dataset_size = len(train_set_combined)  # 훈련 데이터셋 크기\n",
        "\n",
        "T_max = dataset_size / BATCH_SIZE  # 한 주기의 에폭 수\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=T_max, eta_min=0.001)\n",
        "\n",
        "print(\"Created a learning model and optimizer\")"
      ],
      "metadata": {
        "id": "bUwJxdSvasgN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cc99e7c-57b1-4c54-a8b7-cd48b44fb41a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created a learning model and optimizer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Train/Evaluation ###\n",
        "def train(model,train_loader,optimizer,epoch):\n",
        "  model.train()\n",
        "  for i,(image,target) in enumerate(train_loader):\n",
        "    image,target = image.to(DEVICE),target.to(DEVICE)\n",
        "    output = model(image)\n",
        "    optimizer.zero_grad()\n",
        "    # loss func을 어떤 것을 사용할 것인지?\n",
        "    train_loss = F.cross_entropy(output,target).to(DEVICE)\n",
        "\n",
        "    train_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # 스케줄러 업데이트\n",
        "    scheduler.step()\n",
        "\n",
        "    if i%10 ==0:\n",
        "      print(\n",
        "          f'Train Epoch: {epoch} [{i}/{len(train_loader)}]\\tloss: {train_loss.item():6f}')\n",
        "\n",
        "  return train_loss\n",
        "\n",
        "def evaluate(model,val_loader):\n",
        "  model.eval()\n",
        "  eval_loss = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for i,(image,target) in enumerate(val_loader):\n",
        "      image,target = image.to(DEVICE),target.to(DEVICE)\n",
        "      output = model(image)\n",
        "\n",
        "      eval_loss += F.cross_entropy(output,target, reduction='sum').item()\n",
        "      pred = output.max(1,keepdim=True)[1]\n",
        "      correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "  eval_loss /= len(val_loader.dataset)\n",
        "  eval_accuracy = 100*correct / len(val_loader.dataset)\n",
        "  return eval_loss,eval_accuracy"
      ],
      "metadata": {
        "id": "hTFBca1CawX3"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Main ###\n",
        "start = time.time()\n",
        "best = 0\n",
        "\n",
        "train_losses = []  # 훈련 손실을 저장할 목록\n",
        "val_losses = []    # 검증 손실을 저장할 목록\n",
        "val_accuracys = []\n",
        "\n",
        "# 디렉토리 생성\n",
        "os.makedirs('./best_model', exist_ok=True)\n",
        "\n",
        "for epoch in range(EPOCH):\n",
        "  train_loss = train(model,train_loader,optimizer,epoch)\n",
        "  val_loss,val_accuracy = evaluate(model,val_loader)\n",
        "\n",
        "  # 훈련 및 검증 손실을 목록에 추가\n",
        "  train_losses.append(train_loss)\n",
        "  val_losses.append(val_loss)\n",
        "\n",
        "  # Save best model\n",
        "  if val_accuracy > best:\n",
        "    best = val_accuracy\n",
        "    torch.save(model.state_dict(),\"./best_model.pth\")\n",
        "\n",
        "  val_accuracys.append(val_accuracy)\n",
        "  print(f\"[{epoch}]Validation Loss: {val_loss:.4f},Accuracy: {val_accuracy:.4f}%\")\n",
        "\n",
        "# Test result\n",
        "test_loss,test_accuracy = evaluate(model,test_loader)\n",
        "print(f'[FINAL] Test Loss: {test_loss:.4f},Accuracy: {test_accuracy:.4f}%')\n",
        "\n",
        "end = time.time()\n",
        "elasped_time = end - start\n",
        "\n",
        "print(\"Best Accuracy: \",best)\n",
        "print(\n",
        "    f\"Elasped Time: {int(elasped_time/3600)}h, {int(elasped_time/60)}m, {int(elasped_time%60)}s\")\n",
        "print(\n",
        "    f\"time: {int(elasped_time/3600)}h, {int(elasped_time/60)}m, {int(elasped_time%60)}s\")"
      ],
      "metadata": {
        "id": "jwy42e_9a0oW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "25a9d5ae-8b5e-4886-95bd-18030f93dfc2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 0 [0/296]\tloss: 4.128969\n",
            "Train Epoch: 0 [10/296]\tloss: 3.680883\n",
            "Train Epoch: 0 [20/296]\tloss: 4.305448\n",
            "Train Epoch: 0 [30/296]\tloss: 3.522226\n",
            "Train Epoch: 0 [40/296]\tloss: 3.078808\n",
            "Train Epoch: 0 [50/296]\tloss: 2.833106\n",
            "Train Epoch: 0 [60/296]\tloss: 2.849665\n",
            "Train Epoch: 0 [70/296]\tloss: 2.436032\n",
            "Train Epoch: 0 [80/296]\tloss: 2.670633\n",
            "Train Epoch: 0 [90/296]\tloss: 2.504791\n",
            "Train Epoch: 0 [100/296]\tloss: 2.164009\n",
            "Train Epoch: 0 [110/296]\tloss: 2.563940\n",
            "Train Epoch: 0 [120/296]\tloss: 1.947523\n",
            "Train Epoch: 0 [130/296]\tloss: 2.546275\n",
            "Train Epoch: 0 [140/296]\tloss: 2.024667\n",
            "Train Epoch: 0 [150/296]\tloss: 2.335599\n",
            "Train Epoch: 0 [160/296]\tloss: 1.675842\n",
            "Train Epoch: 0 [170/296]\tloss: 2.138474\n",
            "Train Epoch: 0 [180/296]\tloss: 2.268368\n",
            "Train Epoch: 0 [190/296]\tloss: 1.881964\n",
            "Train Epoch: 0 [200/296]\tloss: 2.175472\n",
            "Train Epoch: 0 [210/296]\tloss: 1.767843\n",
            "Train Epoch: 0 [220/296]\tloss: 1.464328\n",
            "Train Epoch: 0 [230/296]\tloss: 1.819427\n",
            "Train Epoch: 0 [240/296]\tloss: 1.889012\n",
            "Train Epoch: 0 [250/296]\tloss: 2.023215\n",
            "Train Epoch: 0 [260/296]\tloss: 1.856838\n",
            "Train Epoch: 0 [270/296]\tloss: 1.759094\n",
            "Train Epoch: 0 [280/296]\tloss: 2.143602\n",
            "Train Epoch: 0 [290/296]\tloss: 1.446774\n",
            "[0]Validation Loss: 3.1533,Accuracy: 34.1216%\n",
            "Train Epoch: 1 [0/296]\tloss: 1.986071\n",
            "Train Epoch: 1 [10/296]\tloss: 2.893804\n",
            "Train Epoch: 1 [20/296]\tloss: 1.895102\n",
            "Train Epoch: 1 [30/296]\tloss: 1.524555\n",
            "Train Epoch: 1 [40/296]\tloss: 1.686493\n",
            "Train Epoch: 1 [50/296]\tloss: 1.600171\n",
            "Train Epoch: 1 [60/296]\tloss: 1.505816\n",
            "Train Epoch: 1 [70/296]\tloss: 1.270862\n",
            "Train Epoch: 1 [80/296]\tloss: 1.130557\n",
            "Train Epoch: 1 [90/296]\tloss: 1.445328\n",
            "Train Epoch: 1 [100/296]\tloss: 1.803437\n",
            "Train Epoch: 1 [110/296]\tloss: 2.358869\n",
            "Train Epoch: 1 [120/296]\tloss: 1.835657\n",
            "Train Epoch: 1 [130/296]\tloss: 0.892468\n",
            "Train Epoch: 1 [140/296]\tloss: 2.014619\n",
            "Train Epoch: 1 [150/296]\tloss: 1.057415\n",
            "Train Epoch: 1 [160/296]\tloss: 0.835849\n",
            "Train Epoch: 1 [170/296]\tloss: 1.208545\n",
            "Train Epoch: 1 [180/296]\tloss: 1.064107\n",
            "Train Epoch: 1 [190/296]\tloss: 1.310531\n",
            "Train Epoch: 1 [200/296]\tloss: 1.229403\n",
            "Train Epoch: 1 [210/296]\tloss: 1.219489\n",
            "Train Epoch: 1 [220/296]\tloss: 1.318230\n",
            "Train Epoch: 1 [230/296]\tloss: 1.479759\n",
            "Train Epoch: 1 [240/296]\tloss: 1.558019\n",
            "Train Epoch: 1 [250/296]\tloss: 1.054422\n",
            "Train Epoch: 1 [260/296]\tloss: 1.190410\n",
            "Train Epoch: 1 [270/296]\tloss: 0.542855\n",
            "Train Epoch: 1 [280/296]\tloss: 1.311969\n",
            "Train Epoch: 1 [290/296]\tloss: 1.224573\n",
            "[1]Validation Loss: 1.2314,Accuracy: 62.5000%\n",
            "Train Epoch: 2 [0/296]\tloss: 0.582337\n",
            "Train Epoch: 2 [10/296]\tloss: 1.205705\n",
            "Train Epoch: 2 [20/296]\tloss: 1.328546\n",
            "Train Epoch: 2 [30/296]\tloss: 1.390974\n",
            "Train Epoch: 2 [40/296]\tloss: 1.503903\n",
            "Train Epoch: 2 [50/296]\tloss: 1.097890\n",
            "Train Epoch: 2 [60/296]\tloss: 1.042592\n",
            "Train Epoch: 2 [70/296]\tloss: 1.120831\n",
            "Train Epoch: 2 [80/296]\tloss: 1.369922\n",
            "Train Epoch: 2 [90/296]\tloss: 1.286887\n",
            "Train Epoch: 2 [100/296]\tloss: 0.823543\n",
            "Train Epoch: 2 [110/296]\tloss: 1.348805\n",
            "Train Epoch: 2 [120/296]\tloss: 0.809494\n",
            "Train Epoch: 2 [130/296]\tloss: 0.910023\n",
            "Train Epoch: 2 [140/296]\tloss: 1.238259\n",
            "Train Epoch: 2 [150/296]\tloss: 0.600457\n",
            "Train Epoch: 2 [160/296]\tloss: 1.356958\n",
            "Train Epoch: 2 [170/296]\tloss: 1.313514\n",
            "Train Epoch: 2 [180/296]\tloss: 0.905758\n",
            "Train Epoch: 2 [190/296]\tloss: 1.552199\n",
            "Train Epoch: 2 [200/296]\tloss: 1.141346\n",
            "Train Epoch: 2 [210/296]\tloss: 0.796010\n",
            "Train Epoch: 2 [220/296]\tloss: 1.176826\n",
            "Train Epoch: 2 [230/296]\tloss: 1.007377\n",
            "Train Epoch: 2 [240/296]\tloss: 1.095007\n",
            "Train Epoch: 2 [250/296]\tloss: 1.044077\n",
            "Train Epoch: 2 [260/296]\tloss: 0.886853\n",
            "Train Epoch: 2 [270/296]\tloss: 0.929385\n",
            "Train Epoch: 2 [280/296]\tloss: 1.125175\n",
            "Train Epoch: 2 [290/296]\tloss: 1.397857\n",
            "[2]Validation Loss: 1.5670,Accuracy: 55.0676%\n",
            "Train Epoch: 3 [0/296]\tloss: 1.012373\n",
            "Train Epoch: 3 [10/296]\tloss: 0.848660\n",
            "Train Epoch: 3 [20/296]\tloss: 0.832144\n",
            "Train Epoch: 3 [30/296]\tloss: 1.232122\n",
            "Train Epoch: 3 [40/296]\tloss: 1.080242\n",
            "Train Epoch: 3 [50/296]\tloss: 1.155433\n",
            "Train Epoch: 3 [60/296]\tloss: 1.049533\n",
            "Train Epoch: 3 [70/296]\tloss: 0.973317\n",
            "Train Epoch: 3 [80/296]\tloss: 1.290512\n",
            "Train Epoch: 3 [90/296]\tloss: 0.878173\n",
            "Train Epoch: 3 [100/296]\tloss: 1.242596\n",
            "Train Epoch: 3 [110/296]\tloss: 0.952116\n",
            "Train Epoch: 3 [120/296]\tloss: 0.713761\n",
            "Train Epoch: 3 [130/296]\tloss: 1.161609\n",
            "Train Epoch: 3 [140/296]\tloss: 0.750004\n",
            "Train Epoch: 3 [150/296]\tloss: 0.813461\n",
            "Train Epoch: 3 [160/296]\tloss: 0.855394\n",
            "Train Epoch: 3 [170/296]\tloss: 1.192350\n",
            "Train Epoch: 3 [180/296]\tloss: 0.979953\n",
            "Train Epoch: 3 [190/296]\tloss: 0.732095\n",
            "Train Epoch: 3 [200/296]\tloss: 1.151422\n",
            "Train Epoch: 3 [210/296]\tloss: 1.117790\n",
            "Train Epoch: 3 [220/296]\tloss: 1.028161\n",
            "Train Epoch: 3 [230/296]\tloss: 1.016272\n",
            "Train Epoch: 3 [240/296]\tloss: 1.209603\n",
            "Train Epoch: 3 [250/296]\tloss: 0.449407\n",
            "Train Epoch: 3 [260/296]\tloss: 0.675964\n",
            "Train Epoch: 3 [270/296]\tloss: 1.030335\n",
            "Train Epoch: 3 [280/296]\tloss: 1.744184\n",
            "Train Epoch: 3 [290/296]\tloss: 0.854736\n",
            "[3]Validation Loss: 1.2081,Accuracy: 67.5676%\n",
            "Train Epoch: 4 [0/296]\tloss: 1.151821\n",
            "Train Epoch: 4 [10/296]\tloss: 1.226367\n",
            "Train Epoch: 4 [20/296]\tloss: 0.871749\n",
            "Train Epoch: 4 [30/296]\tloss: 0.976352\n",
            "Train Epoch: 4 [40/296]\tloss: 0.503012\n",
            "Train Epoch: 4 [50/296]\tloss: 1.001167\n",
            "Train Epoch: 4 [60/296]\tloss: 0.508013\n",
            "Train Epoch: 4 [70/296]\tloss: 0.641664\n",
            "Train Epoch: 4 [80/296]\tloss: 1.228201\n",
            "Train Epoch: 4 [90/296]\tloss: 0.769758\n",
            "Train Epoch: 4 [100/296]\tloss: 0.530883\n",
            "Train Epoch: 4 [110/296]\tloss: 1.168799\n",
            "Train Epoch: 4 [120/296]\tloss: 1.034894\n",
            "Train Epoch: 4 [130/296]\tloss: 0.955863\n",
            "Train Epoch: 4 [140/296]\tloss: 0.989846\n",
            "Train Epoch: 4 [150/296]\tloss: 0.913501\n",
            "Train Epoch: 4 [160/296]\tloss: 0.759153\n",
            "Train Epoch: 4 [170/296]\tloss: 0.802938\n",
            "Train Epoch: 4 [180/296]\tloss: 1.312770\n",
            "Train Epoch: 4 [190/296]\tloss: 0.563784\n",
            "Train Epoch: 4 [200/296]\tloss: 1.167827\n",
            "Train Epoch: 4 [210/296]\tloss: 1.030200\n",
            "Train Epoch: 4 [220/296]\tloss: 0.805217\n",
            "Train Epoch: 4 [230/296]\tloss: 0.701105\n",
            "Train Epoch: 4 [240/296]\tloss: 0.903237\n",
            "Train Epoch: 4 [250/296]\tloss: 1.264161\n",
            "Train Epoch: 4 [260/296]\tloss: 1.007108\n",
            "Train Epoch: 4 [270/296]\tloss: 1.063461\n",
            "Train Epoch: 4 [280/296]\tloss: 0.896263\n",
            "Train Epoch: 4 [290/296]\tloss: 0.784909\n",
            "[4]Validation Loss: 1.1555,Accuracy: 71.6216%\n",
            "Train Epoch: 5 [0/296]\tloss: 0.807309\n",
            "Train Epoch: 5 [10/296]\tloss: 1.175549\n",
            "Train Epoch: 5 [20/296]\tloss: 0.862701\n",
            "Train Epoch: 5 [30/296]\tloss: 1.177981\n",
            "Train Epoch: 5 [40/296]\tloss: 1.178344\n",
            "Train Epoch: 5 [50/296]\tloss: 1.029772\n",
            "Train Epoch: 5 [60/296]\tloss: 0.605062\n",
            "Train Epoch: 5 [70/296]\tloss: 1.173454\n",
            "Train Epoch: 5 [80/296]\tloss: 1.044097\n",
            "Train Epoch: 5 [90/296]\tloss: 1.077974\n",
            "Train Epoch: 5 [100/296]\tloss: 0.471447\n",
            "Train Epoch: 5 [110/296]\tloss: 0.800894\n",
            "Train Epoch: 5 [120/296]\tloss: 0.784321\n",
            "Train Epoch: 5 [130/296]\tloss: 0.778724\n",
            "Train Epoch: 5 [140/296]\tloss: 0.864437\n",
            "Train Epoch: 5 [150/296]\tloss: 0.682936\n",
            "Train Epoch: 5 [160/296]\tloss: 0.687984\n",
            "Train Epoch: 5 [170/296]\tloss: 0.897366\n",
            "Train Epoch: 5 [180/296]\tloss: 1.206930\n",
            "Train Epoch: 5 [190/296]\tloss: 1.244535\n",
            "Train Epoch: 5 [200/296]\tloss: 1.018285\n",
            "Train Epoch: 5 [210/296]\tloss: 1.203681\n",
            "Train Epoch: 5 [220/296]\tloss: 1.051389\n",
            "Train Epoch: 5 [230/296]\tloss: 0.577814\n",
            "Train Epoch: 5 [240/296]\tloss: 0.844257\n",
            "Train Epoch: 5 [250/296]\tloss: 1.020802\n",
            "Train Epoch: 5 [260/296]\tloss: 0.564940\n",
            "Train Epoch: 5 [270/296]\tloss: 1.266770\n",
            "Train Epoch: 5 [280/296]\tloss: 0.897089\n",
            "Train Epoch: 5 [290/296]\tloss: 0.741465\n",
            "[5]Validation Loss: 1.4915,Accuracy: 61.4865%\n",
            "Train Epoch: 6 [0/296]\tloss: 0.669271\n",
            "Train Epoch: 6 [10/296]\tloss: 0.715862\n",
            "Train Epoch: 6 [20/296]\tloss: 0.952080\n",
            "Train Epoch: 6 [30/296]\tloss: 0.709572\n",
            "Train Epoch: 6 [40/296]\tloss: 1.380060\n",
            "Train Epoch: 6 [50/296]\tloss: 1.081281\n",
            "Train Epoch: 6 [60/296]\tloss: 0.576042\n",
            "Train Epoch: 6 [70/296]\tloss: 0.948892\n",
            "Train Epoch: 6 [80/296]\tloss: 1.127037\n",
            "Train Epoch: 6 [90/296]\tloss: 0.666614\n",
            "Train Epoch: 6 [100/296]\tloss: 1.328521\n",
            "Train Epoch: 6 [110/296]\tloss: 0.786150\n",
            "Train Epoch: 6 [120/296]\tloss: 0.783414\n",
            "Train Epoch: 6 [130/296]\tloss: 0.478986\n",
            "Train Epoch: 6 [140/296]\tloss: 0.822924\n",
            "Train Epoch: 6 [150/296]\tloss: 1.306575\n",
            "Train Epoch: 6 [160/296]\tloss: 1.676886\n",
            "Train Epoch: 6 [170/296]\tloss: 0.991926\n",
            "Train Epoch: 6 [180/296]\tloss: 0.774076\n",
            "Train Epoch: 6 [190/296]\tloss: 0.721478\n",
            "Train Epoch: 6 [200/296]\tloss: 0.881352\n",
            "Train Epoch: 6 [210/296]\tloss: 0.753799\n",
            "Train Epoch: 6 [220/296]\tloss: 1.005880\n",
            "Train Epoch: 6 [230/296]\tloss: 1.244122\n",
            "Train Epoch: 6 [240/296]\tloss: 0.594522\n",
            "Train Epoch: 6 [250/296]\tloss: 1.043952\n",
            "Train Epoch: 6 [260/296]\tloss: 1.762855\n",
            "Train Epoch: 6 [270/296]\tloss: 0.927007\n",
            "Train Epoch: 6 [280/296]\tloss: 0.698453\n",
            "Train Epoch: 6 [290/296]\tloss: 0.803144\n",
            "[6]Validation Loss: 1.7629,Accuracy: 63.1757%\n",
            "Train Epoch: 7 [0/296]\tloss: 0.744162\n",
            "Train Epoch: 7 [10/296]\tloss: 0.947527\n",
            "Train Epoch: 7 [20/296]\tloss: 0.934349\n",
            "Train Epoch: 7 [30/296]\tloss: 0.971860\n",
            "Train Epoch: 7 [40/296]\tloss: 0.662498\n",
            "Train Epoch: 7 [50/296]\tloss: 1.046178\n",
            "Train Epoch: 7 [60/296]\tloss: 0.850046\n",
            "Train Epoch: 7 [70/296]\tloss: 1.104326\n",
            "Train Epoch: 7 [80/296]\tloss: 0.876201\n",
            "Train Epoch: 7 [90/296]\tloss: 0.788513\n",
            "Train Epoch: 7 [100/296]\tloss: 0.567597\n",
            "Train Epoch: 7 [110/296]\tloss: 0.773559\n",
            "Train Epoch: 7 [120/296]\tloss: 0.958342\n",
            "Train Epoch: 7 [130/296]\tloss: 0.817971\n",
            "Train Epoch: 7 [140/296]\tloss: 0.975666\n",
            "Train Epoch: 7 [150/296]\tloss: 0.922919\n",
            "Train Epoch: 7 [160/296]\tloss: 0.594019\n",
            "Train Epoch: 7 [170/296]\tloss: 0.493433\n",
            "Train Epoch: 7 [180/296]\tloss: 0.941516\n",
            "Train Epoch: 7 [190/296]\tloss: 0.588160\n",
            "Train Epoch: 7 [200/296]\tloss: 0.695357\n",
            "Train Epoch: 7 [210/296]\tloss: 0.756354\n",
            "Train Epoch: 7 [220/296]\tloss: 1.236908\n",
            "Train Epoch: 7 [230/296]\tloss: 0.930835\n",
            "Train Epoch: 7 [240/296]\tloss: 0.795240\n",
            "Train Epoch: 7 [250/296]\tloss: 0.699711\n",
            "Train Epoch: 7 [260/296]\tloss: 0.513588\n",
            "Train Epoch: 7 [270/296]\tloss: 0.605050\n",
            "Train Epoch: 7 [280/296]\tloss: 0.418723\n",
            "Train Epoch: 7 [290/296]\tloss: 0.657836\n",
            "[7]Validation Loss: 1.3709,Accuracy: 65.2027%\n",
            "Train Epoch: 8 [0/296]\tloss: 0.808560\n",
            "Train Epoch: 8 [10/296]\tloss: 0.828962\n",
            "Train Epoch: 8 [20/296]\tloss: 0.434239\n",
            "Train Epoch: 8 [30/296]\tloss: 0.724059\n",
            "Train Epoch: 8 [40/296]\tloss: 0.615645\n",
            "Train Epoch: 8 [50/296]\tloss: 0.274818\n",
            "Train Epoch: 8 [60/296]\tloss: 0.923416\n",
            "Train Epoch: 8 [70/296]\tloss: 1.016634\n",
            "Train Epoch: 8 [80/296]\tloss: 0.699300\n",
            "Train Epoch: 8 [90/296]\tloss: 0.943584\n",
            "Train Epoch: 8 [100/296]\tloss: 0.509293\n",
            "Train Epoch: 8 [110/296]\tloss: 0.558817\n",
            "Train Epoch: 8 [120/296]\tloss: 0.762186\n",
            "Train Epoch: 8 [130/296]\tloss: 0.608129\n",
            "Train Epoch: 8 [140/296]\tloss: 0.750490\n",
            "Train Epoch: 8 [150/296]\tloss: 1.070642\n",
            "Train Epoch: 8 [160/296]\tloss: 0.618396\n",
            "Train Epoch: 8 [170/296]\tloss: 0.869010\n",
            "Train Epoch: 8 [180/296]\tloss: 0.604802\n",
            "Train Epoch: 8 [190/296]\tloss: 1.274912\n",
            "Train Epoch: 8 [200/296]\tloss: 0.961906\n",
            "Train Epoch: 8 [210/296]\tloss: 1.121494\n",
            "Train Epoch: 8 [220/296]\tloss: 1.037076\n",
            "Train Epoch: 8 [230/296]\tloss: 0.596066\n",
            "Train Epoch: 8 [240/296]\tloss: 1.122388\n",
            "Train Epoch: 8 [250/296]\tloss: 0.628261\n",
            "Train Epoch: 8 [260/296]\tloss: 0.564478\n",
            "Train Epoch: 8 [270/296]\tloss: 1.071617\n",
            "Train Epoch: 8 [280/296]\tloss: 0.710164\n",
            "Train Epoch: 8 [290/296]\tloss: 0.806590\n",
            "[8]Validation Loss: 1.3436,Accuracy: 64.8649%\n",
            "Train Epoch: 9 [0/296]\tloss: 0.989595\n",
            "Train Epoch: 9 [10/296]\tloss: 0.482586\n",
            "Train Epoch: 9 [20/296]\tloss: 0.847989\n",
            "Train Epoch: 9 [30/296]\tloss: 0.840140\n",
            "Train Epoch: 9 [40/296]\tloss: 0.722131\n",
            "Train Epoch: 9 [50/296]\tloss: 0.980086\n",
            "Train Epoch: 9 [60/296]\tloss: 0.480088\n",
            "Train Epoch: 9 [70/296]\tloss: 0.802854\n",
            "Train Epoch: 9 [80/296]\tloss: 0.791242\n",
            "Train Epoch: 9 [90/296]\tloss: 0.195442\n",
            "Train Epoch: 9 [100/296]\tloss: 0.901956\n",
            "Train Epoch: 9 [110/296]\tloss: 0.802459\n",
            "Train Epoch: 9 [120/296]\tloss: 0.551752\n",
            "Train Epoch: 9 [130/296]\tloss: 0.800656\n",
            "Train Epoch: 9 [140/296]\tloss: 0.873833\n",
            "Train Epoch: 9 [150/296]\tloss: 0.838331\n",
            "Train Epoch: 9 [160/296]\tloss: 1.027014\n",
            "Train Epoch: 9 [170/296]\tloss: 0.595159\n",
            "Train Epoch: 9 [180/296]\tloss: 0.813468\n",
            "Train Epoch: 9 [190/296]\tloss: 0.853629\n",
            "Train Epoch: 9 [200/296]\tloss: 0.771967\n",
            "Train Epoch: 9 [210/296]\tloss: 0.958035\n",
            "Train Epoch: 9 [220/296]\tloss: 0.401438\n",
            "Train Epoch: 9 [230/296]\tloss: 0.475830\n",
            "Train Epoch: 9 [240/296]\tloss: 1.271154\n",
            "Train Epoch: 9 [250/296]\tloss: 1.242630\n",
            "Train Epoch: 9 [260/296]\tloss: 0.610002\n",
            "Train Epoch: 9 [270/296]\tloss: 0.135759\n",
            "Train Epoch: 9 [280/296]\tloss: 0.400910\n",
            "Train Epoch: 9 [290/296]\tloss: 0.888921\n",
            "[9]Validation Loss: 1.7112,Accuracy: 64.5270%\n",
            "Train Epoch: 10 [0/296]\tloss: 1.175091\n",
            "Train Epoch: 10 [10/296]\tloss: 1.130195\n",
            "Train Epoch: 10 [20/296]\tloss: 0.980268\n",
            "Train Epoch: 10 [30/296]\tloss: 0.564933\n",
            "Train Epoch: 10 [40/296]\tloss: 0.801514\n",
            "Train Epoch: 10 [50/296]\tloss: 0.996948\n",
            "Train Epoch: 10 [60/296]\tloss: 1.005936\n",
            "Train Epoch: 10 [70/296]\tloss: 0.773134\n",
            "Train Epoch: 10 [80/296]\tloss: 0.649215\n",
            "Train Epoch: 10 [90/296]\tloss: 0.918604\n",
            "Train Epoch: 10 [100/296]\tloss: 0.915038\n",
            "Train Epoch: 10 [110/296]\tloss: 0.948840\n",
            "Train Epoch: 10 [120/296]\tloss: 0.939179\n",
            "Train Epoch: 10 [130/296]\tloss: 0.872860\n",
            "Train Epoch: 10 [140/296]\tloss: 1.356669\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-2c9236cfa60c>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m   \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m   \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-49c5f3ad07cd>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, optimizer, epoch)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    675\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Handle `CustomType` automatically\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \"\"\"\n\u001b[0;32m--> 265\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_collate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcollate_fn_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0melem_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcollate_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_typed_storage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 이제 손실 값을 플로팅합니다.\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# train_loss와 val_loss 텐서를 CPU로 이동하고 NumPy 배열로 변환\n",
        "train_losses = [loss.cpu().detach().numpy() if isinstance(loss, torch.Tensor) else loss for loss in train_losses]\n",
        "val_losses = [loss.cpu().detach().numpy() if isinstance(loss, torch.Tensor) else loss for loss in val_losses]\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(range(1, EPOCH + 1), train_losses, label='train_loss', marker='o')\n",
        "plt.plot(range(1, EPOCH + 1), val_losses, label='val_loss', marker='o')\n",
        "plt.title('Training and Validation Loss Over Epochs')\n",
        "\n",
        "plt.xlabel('EPOCH')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "val_accuracys = [acc.cpu().detach().numpy() if isinstance(acc, torch.Tensor) else acc for acc in val_accuracys]\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(range(1, EPOCH + 1), val_accuracys, label='val_accuracy', marker='o')\n",
        "plt.title('Validation Accuracy Over Epochs')\n",
        "\n",
        "plt.xlabel('EPOCH')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Cg1gbuDya5Mt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}